{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#1 : SETUP & LIBRARIES"
   ],
   "metadata": {
    "id": "I9TOC8Wf8Nj1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ga6vZS2P7YSE",
    "outputId": "33ebf8ba-647f-4313-d429-2da634ef0ede"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "Current compute device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive to save the final model\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Current compute device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 : Merging method's (SLERP) LOGIC"
   ],
   "metadata": {
    "id": "Xr49tWEt8V1y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This cell contains the mathematical implementation of SLERP.\n",
    "\n",
    "def lerp(t: float, v0: np.ndarray, v1: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Linear Interpolation (LERP).\n",
    "\n",
    "    Used as a fallback when vectors are colinear (pointing in same direction).\n",
    "    Formula: (1 - t) * v0 + t * v1\n",
    "\n",
    "    Args:\n",
    "        t (float): Interpolation factor [0, 1].\n",
    "        v0 (np.ndarray): Starting vector.\n",
    "        v1 (np.ndarray): Target vector.\n",
    "    \"\"\"\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "def normalize(v: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes a vector to unit length.\n",
    "\n",
    "    In high-dimensional spaces (like LLM weights), magnitude represents\n",
    "    significance. Normalizing allows us to compare 'directions' of features\n",
    "    independent of their strength.\n",
    "\n",
    "    Args:\n",
    "        v (np.ndarray): Input vector.\n",
    "        eps (float): Epsilon value to prevent division by zero errors.\n",
    "    \"\"\"\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_v > eps:\n",
    "        v = v / norm_v\n",
    "    return v\n",
    "\n",
    "def slerp(\n",
    "    t: float,\n",
    "    v0: torch.Tensor,\n",
    "    v1: torch.Tensor,\n",
    "    dot_threshold: float = 0.9995,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Spherical Linear Interpolation (SLERP).\n",
    "\n",
    "    Unlike LERP, which cuts a straight line through the sphere (shortening vectors),\n",
    "    SLERP follows the curvature of the sphere. This preserves the 'magnitude'\n",
    "    (variance) of the weights, which is crucial for maintaining LLM stability.\n",
    "\n",
    "    Args:\n",
    "        t (float): The mixing factor.\n",
    "                   0.0 = 100% Model A, 1.0 = 100% Model B.\n",
    "        v0 (torch.Tensor): Weights from Model A.\n",
    "        v1 (torch.Tensor): Weights from Model B.\n",
    "        dot_threshold (float): Threshold to decide when vectors are parallel.\n",
    "                               If dot product > 0.9995, we switch to LERP for stability.\n",
    "        eps (float): Epsilon for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The merged weight tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Convert PyTorch Tensors to NumPy\n",
    "    # We move to CPU and convert to float32 (numpy default) for high-precision math.\n",
    "    # LLM weights are often Float16/Bfloat16, but interpolation requires Float32 to avoid rounding errors destroying the model.\n",
    "    v0_np = v0.detach().cpu().float().numpy()\n",
    "    v1_np = v1.detach().cpu().float().numpy()\n",
    "\n",
    "    # 2. Copy vectors to preserve original magnitudes for the final reconstruction\n",
    "    v0_copy = np.copy(v0_np)\n",
    "    v1_copy = np.copy(v1_np)\n",
    "\n",
    "    # 3. Normalize vectors to unit sphere to calculate angles\n",
    "    v0_np = normalize(v0_np, eps)\n",
    "    v1_np = normalize(v1_np, eps)\n",
    "\n",
    "    # 4. Calculate the Dot Product (Cosine of angle theta)\n",
    "    # This tells us how \"close\" the two models are in feature space.\n",
    "    dot = np.sum(v0_np * v1_np)\n",
    "\n",
    "    # 5. Handle Alignment (Colinearity)\n",
    "    # If the vectors are almost identical (dot close to 1), SLERP calculation\n",
    "    # involves division by sin(0), which is unstable. We fallback to LERP.\n",
    "    if np.abs(dot) > dot_threshold:\n",
    "        res = lerp(t, v0_copy, v1_copy)\n",
    "        return torch.from_numpy(res).to(v0.dtype).to(v0.device)\n",
    "\n",
    "    # 6. Calculate Angles (Theta)\n",
    "    # arccos(dot) gives us the angle between the two vectors\n",
    "    theta_0 = np.arccos(dot)\n",
    "    sin_theta_0 = np.sin(theta_0)\n",
    "\n",
    "    # Calculate the angle at the interpolation point t\n",
    "    theta_t = theta_0 * t\n",
    "    sin_theta_t = np.sin(theta_t)\n",
    "\n",
    "    # 7. Apply the SLERP Formula\n",
    "    # factor0 scales the contribution of v0 based on angle\n",
    "    # factor1 scales the contribution of v1 based on angle\n",
    "    s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "    s1 = sin_theta_t / sin_theta_0\n",
    "\n",
    "    # Reconstruct the vector using the original magnitudes (v0_copy, v1_copy)\n",
    "    res = s0 * v0_copy + s1 * v1_copy\n",
    "\n",
    "    # 8. Convert back to PyTorch Tensor with original dtype/device\n",
    "    return torch.from_numpy(res).to(v0.dtype).to(v0.device)"
   ],
   "metadata": {
    "id": "iItoYcjE7pDH"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 : THE MERGE EXECUTION"
   ],
   "metadata": {
    "id": "_2VrsJQq8kK-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def merge_models(model_path_a, model_path_b, output_path, merge_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Loads two models, applies SLERP layer-by-layer, and saves the result.\n",
    "    \"\"\"\n",
    "    print(f\"Loading Model A: {model_path_a}...\")\n",
    "    # Loading map_location='cpu' to spare GPU VRAM for calculations if needed\n",
    "    # Note: Loading two 7B models requires ~28GB CPU RAM.\n",
    "    model_a = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_a,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"cpu\"\n",
    "    )\n",
    "\n",
    "    print(f\"Loading Model B: {model_path_b}...\")\n",
    "    model_b = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_b,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"cpu\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path_a)\n",
    "\n",
    "    print(f\"Starting SLERP Merge with t={merge_ratio}...\")\n",
    "\n",
    "    # We iterate over the state dictionary of Model A\n",
    "    # The assumption is Model A and B have identical architectures (layer names)\n",
    "    model_a_dict = model_a.state_dict()\n",
    "    model_b_dict = model_b.state_dict()\n",
    "\n",
    "    merged_dict = {}\n",
    "\n",
    "    for key in model_a_dict.keys():\n",
    "        # Skip keys that might track metrics like 'num_batches_tracked'\n",
    "        if key not in model_b_dict:\n",
    "            print(f\"Skipping {key}: Not found in Model B\")\n",
    "            continue\n",
    "\n",
    "        tensor_a = model_a_dict[key]\n",
    "        tensor_b = model_b_dict[key]\n",
    "\n",
    "        # Check shapes\n",
    "        if tensor_a.shape != tensor_b.shape:\n",
    "            # Sometimes vocab sizes differ slightly, we usually take the larger one\n",
    "            # or raise an error. For this specific merge, we assume compatibility.\n",
    "            print(f\"Shape mismatch at {key}: {tensor_a.shape} vs {tensor_b.shape}. Skipping.\")\n",
    "            merged_dict[key] = tensor_a\n",
    "            continue\n",
    "\n",
    "        # Apply SLERP to weight tensors\n",
    "        # We skip integers (like bias terms usually are floats, but position IDs are ints)\n",
    "        if \"int\" in str(tensor_a.dtype):\n",
    "            merged_dict[key] = tensor_a\n",
    "        else:\n",
    "            # The core magic happens here\n",
    "            merged_dict[key] = slerp(merge_ratio, tensor_a, tensor_b)\n",
    "\n",
    "        # Garbage collection to free up RAM during the loop\n",
    "        if key in model_b_dict:\n",
    "            del model_b_dict[key]\n",
    "\n",
    "    # Clear memory of Model B completely\n",
    "    del model_b\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Merge complete. Saving model...\")\n",
    "\n",
    "    # Load the merged weights back into Model A structure\n",
    "    model_a.load_state_dict(merged_dict)\n",
    "\n",
    "    # Save to Drive\n",
    "    model_a.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "\n",
    "    print(f\"Model successfully saved to: {output_path}\")"
   ],
   "metadata": {
    "id": "wFWEsvmE71KZ"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 : CONFIGURATION & RUN"
   ],
   "metadata": {
    "id": "9J2hs3K78pHo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "MODEL_1_URL = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "MODEL_2_URL = \"Gryphe/MythoMist-7b\"\n",
    "MERGED_NAME = \"MyMergedModel01-7B\"\n",
    "\n",
    "# Where to save on Google Drive\n",
    "OUTPUT_DIR = f\"/content/drive/MyDrive/{MERGED_NAME}\"\n",
    "\n",
    "# The SLERP Factor (t)\n",
    "# 0.5 means equal mix.\n",
    "# 0.7 means 30% Model A, 70% Model B.\n",
    "MERGE_RATIO = 0.5\n",
    "\n",
    "# Run the Merge\n",
    "# This will download the models, merge them, and upload to your Drive.\n",
    "try:\n",
    "    merge_models(MODEL_1_URL, MODEL_2_URL, OUTPUT_DIR, MERGE_RATIO)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"If you ran out of RAM (OOM), try using a High-RAM runtime.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883,
     "referenced_widgets": [
      "994798bbf8c5415f8afa8d01ca202a0e",
      "c4dcfe61e12446da8f0dfd7a2b720450",
      "fc609c18e7d046f59dcc02194d282500",
      "715bc37221fe4503a55355eec4d4acf6",
      "0f2e74db270248bc996682c9c9d89cf7",
      "b2889396da5741d88b94bb0660ce1744",
      "f7503ba4769f4f8eae25b6e04224186f",
      "71393dd08300456b80d2346944d45673",
      "233924337baf4fe283d7fa5ce0c86574",
      "68e83b078e2145529023627152c58be4",
      "18af7831043742118716b8a5c384fb85",
      "334e922a41c243278d93d665ff759145",
      "2d35594abde3427bb8d8230f7524b938",
      "9872598bf26c455bb54c2ae154060521",
      "6f4a2e4a902d44d3b8a4b74af31f37a7",
      "7833c7ab33694e5ca452c45028feabdd",
      "f9a4f85336624208bd82cbdc9a78cd00",
      "69356056453345929c736db545200096",
      "a49c4227d30c4b81b579348c1a0a890e",
      "3b8cbe77d2084377b2e203fad5b41d41",
      "3949465d47bb4bf8a247cbd6d729dbd0",
      "43e83bc988f94fb7964861e8ee7d93e3",
      "97c1510cd91c4a7e901ba4c0f24ec818",
      "2224066b8a154df4863ad50f44bcebc1",
      "c066902d41ce4a709c26052cd9cddc62",
      "f5443d7d5aa142739b32cd67cc134ea4",
      "13b88082757c47989f45c2789e5744d9",
      "b1f5e0f5de47421f960dd31f699cc4c9",
      "a09c2258bf2b4d7b9a5bf04212e6281f",
      "c6ad1f0d28374c83b847ff3870a6e4ea",
      "a0541b00abf34bf8b60f52edcad98155",
      "83b5f09f8a074e238dae3671736fc622",
      "6c172bec99ce4e87962e8c2c9a80188c",
      "ee2b3ceae48444b6bdaf81254108f703",
      "6d2c25363be641adb09857c95e6f126b",
      "743fecb8c21b4c95a4b3a492a6f32262",
      "ea01e12fd7a544d9aa0f8e9bcb317917",
      "983654889e5346fba017979d7f8f81ec",
      "cb6338a0c76741a3bf4e550b3db1a0bf",
      "b2e3a2a194364b8b86ae5a8353865767",
      "0e10d2cf622d4b459938d8359cff034f",
      "2fe29d96249e453ebdc76f696147c127",
      "8d95d1ac348f4ca885daf3cd929db0ad",
      "e81d068afb854fa68c7af91fbdaf27e6",
      "6897a8bf34b64acb98ce29269a4c19bd",
      "238765ec3dc84edd99ed1a7ef98064ef",
      "78a9921769da4b67aa0ace8a62c4923c",
      "73e78fd66bb2458c96177abf22eac354",
      "dd352d11144848f3954bbf1e6b8b96ad",
      "7aae457800884f98aac0d3321d74c78d",
      "22bc5dcd92774b5f989aba777e36cfb4",
      "200546d6f613498eae565545b258f2d0",
      "4a2764166733437cac766af39b722074",
      "47ff07a81a0346b986463301f502971c",
      "60da4a6e655249888c334eaf3a0c59ea",
      "c3f061b4e75141f7986cf56103b5157d",
      "0f4f9ab05c0c4ee8a067fa41dfa86da3",
      "ec175c61ae294c3ca0a6f6b9d63fb509",
      "231e61a53eed4a98ae7f23790c18449c",
      "a791d7ea7a2b41538219603159d44fa2",
      "a4757a18a3e54a58938554c3fdf4fb8d",
      "76e834b60653426fbdfcd55117acf7f3",
      "a8d646ac0de246669846219c891a6a17",
      "b9f69352f8be46d49f882ba973923994",
      "415333cfacea4cfe8a131b76801a3657",
      "ffad33cf9ff94233af2ffd28f3fb26da",
      "1f743e0ea37d4cc8b822d870567ec4a3",
      "7c3eea03a80c482a879c9f33d5aa8b0c",
      "7acf54fcb7174fafbb15b59b11df3a59",
      "73a4007861c84b80a0b3e0fe106841b0",
      "fd2ed83bd3874122a4b3afd91ccc240c",
      "9e5703904d7c491cb8949fc7e4e91b74",
      "20f0ac77324d4a798ab686b9b8bdc6b6",
      "a0e2687932f446e882bb4f161cb6fd46",
      "b8c4d3cbe4764aceb9a00f58ac9a3214",
      "efbaa5251e2242328c91c7d8fc35d7c7",
      "2c37c0f08d974ba29e8d839db15ee411",
      "747fc5c42ddc4646be78f52783397bba",
      "7f93ac5fa32242fbabfe7c8cfc521d59",
      "4e3b9f8a9a324766b9c2f5640e7d9a11",
      "688ff01c8d2344cda30430b773dd3a64",
      "a55cb5f3b731462bad190dedd6cb2fae",
      "f1db2d9eb13643afa0fae702d728c407",
      "36c7a34e246845c28acadc171e1d0181",
      "b981fca60d734c1083168597ebfc4a47",
      "a3fe10a61def41ab8985ae5fe55fb11b",
      "494a4f3f41b8409388f13f61401201a1",
      "ba0300cfd65f414c8dd32bd7056cd858",
      "8ef1082b288748e5a1cf2ec963a7a7a9",
      "d0435c78824741d099ea6fc48214fec8",
      "02b5a6d2a0594b5788bb2234dbe82dd4",
      "4882b9ada6eb42b5a24415a2b6a9479f",
      "a7cf56ded60e49b88548f40a4629a878",
      "180554cb29924a4a90e777c7352e6307",
      "17513438940e40e9841a88aa9baa8d00",
      "4db462d8939944fd9ba06afff2a8829f",
      "faa16d57e77f4ae3a0aea42e33cfe948",
      "9a781c38bcaa4b1ba7cb7c0642031698",
      "9477b26abb1341b8995607f1aaeaf615",
      "77326c120bd54f0b9c516db082199455",
      "4c5b8645e1a042b99147e5d30c4d68aa",
      "18d3d642a1934bc8b07b48e7ea7ebb1a",
      "e07b1fbffa7849448f4bb6f27005e626",
      "7ab16025a3b14de883aeb9bdf0704746",
      "c84d91ee857f46deaaca553b71b9020c",
      "8a893211293049978035c7e345b0a9f2",
      "1696db25d8e14e2dafd5a1b168366ac1",
      "58064eb6c6064e43b2c1b1759af42926",
      "8a33633f1fc740909fc30769f8e6f461",
      "459ded71cd664926b9ebbb79d9fe0c32",
      "4990e6b7106a427aab10110662c3d495",
      "49837be868b446fabcef61cae5ed47ab",
      "563efff7613b4ba09f7d953b562f89b4",
      "c29fe57b5a88419684230bf4d11742d8",
      "77c669ef0565453695604b97f71a59cb",
      "440ba0903a904ad49b3922768d63e7b0",
      "dbd705ca6281450f977263eeb1b3ad8c",
      "80ccf85edc4a49e9a9d89e3bdea3fe51",
      "1dbc71cc5e854f9d959c207a013dbb29",
      "cf035afe05c648b888057a6187a06e51",
      "e1e5eee547374fb2a2bd5d6362dbee05",
      "11d01e6bd20c42f9bce0541374b3fe17",
      "1f05ddf152b143d4b9e8694270301fda",
      "252d71f349c042478a335c0d770f8caa",
      "4b562cfe832c49d8803d17a18521d968",
      "5149052b89d24d9b95a346cd3cc59350",
      "e8e56bae265b4029820f711ad17f0920",
      "26cff777c1a74d3bbe29f377351fe201",
      "7b951ee18a53411191917cf33905ae29",
      "fc3ec01e738346c3aa0ce54139e44498",
      "39ba1f9eb2784536accfcd54d6c2347f",
      "11ea9779df934bc99edcde579afd3a96",
      "37def251d9d24d24b399acc7c7e97811",
      "29aed35d2b2e485b97eadb4aa16aad94",
      "d9edb4f3cd2b4df788c6ef75052d8045",
      "d285317fdd46464ea2a95187ee4bd08d",
      "a3167164785b4b36b22ab536eab5cbbc",
      "3ff7615f73494b56beb7149af208b769",
      "b35b2fcb15cc47e99e5c4d61a8883555",
      "2820c49a55b249378ac81876b732b7ed",
      "3d132fec56ef4f29b9d4acfb14469cbb",
      "2ef6cd6e558c4c648f1a2c4a7971a37e",
      "317ad31f3a3342698dc0b72f23d50675",
      "90bdb3f2d9cc4c6f845bb57a67c5abb8",
      "5856922ce0a9413d81da1f7e313d367e",
      "31bfb718f67b466f9fb746aee8767f10",
      "f33aa0b4ff874324baff102d1f025b33",
      "fb497b6f519045af9cb46551eaaa96cf",
      "f4687dba8445490a8f8c1e0a6d26e46d",
      "dba5ca00df96453899cd6c4471e61d02",
      "147a1ef5b88542a28eb2790388628039",
      "108b5a694a104efb9ce16df8148d9c43",
      "f506adabfd724acd8b5e5adaee7b6136",
      "9f73806f93b84b019beeb05431823cb3",
      "e06d3776e0fa4b0ca5ccd5b9e4193138",
      "5f12325b5814496ca2f3ab6f392fb5eb",
      "6d9a7009d2304533ba71bcc4e05d73e8",
      "db56816bc6b645f8950efa234b46b3ce",
      "fc532758b0874a44a877b55324c27db9",
      "5bfa2ba2912a4ace8da1a1ce6d650e2c",
      "d705a90487944f83a5b44278b9e3b741",
      "9d7ff892ef6742e6a2be77aad59fec5d",
      "de8cda3ac5ad4dddb1b3c2c8b40f776c",
      "97f7ced5564e4f5ea1912215148efba6",
      "0c813531bc9a42ed99040285a4f6677b",
      "29ba1e10bae74e8fa1d19d8d480d5e69",
      "b0dbb654aa244aedb21a0f4c8fd0a176",
      "565c366cf52c495bbebe709474599f05",
      "4c4fb1fe81c045119b78c9a6b1a0b561",
      "ef26b0d1e15343b6a5a847086a1194ec",
      "4bc8b9a01d454a749970118e09953b68",
      "2319ca0679104c779e757cacf9dbe01e",
      "533fcea790e04bcda8fc3c8914b7bbaf",
      "fd858462cd8940b0ab67d38c7a388025",
      "5ddfa9c5e0b8478eb9b1ca93b0f6eec5",
      "44717b5280284828b94a1ddd44fe4c06",
      "a0b8f1eaa94040e18f827ecd2ca6b18f",
      "f0b83085514b46d195b7967577c3488b",
      "971ff142e7f54c54aca84b7ec821d930",
      "3314a76b112f4d2ea22c91fd8d0d33b4",
      "43f0ab50ea614464af3566605e66e46d",
      "d6eace42c9554a0eb3fbd0fe4aa1d88f",
      "07e40f143aaa4d3c8dd6381f3fac6eff",
      "681805d301cf4182900da19cc5f8c36f",
      "8e732b4fced1467d91c7da1399228404",
      "a63ba16b1d0d40a3b5452a8869506a54",
      "8367ea21196f4d348dcb13c73b1363b8",
      "42a67f2e26bb484db8901bf463facccf",
      "2bfa64fc4a794e3cb2c67c8f3e8cc558",
      "1251a70253554f24b9ea6ecdf4b88e66",
      "0d5d13f088df40fe83aedf3530c7aa47",
      "a65a10295a004f57b16a9c176bdd0afe",
      "d1fe2ec9668f4b448c173f8e74fd494b",
      "ae5ca9f5752a4be484207a636113d0dc",
      "00890c25ec3f4ba2b506c9060e1df3a4",
      "37c622d03b4c4e1bbf9a22210fc6292f",
      "be92c49780e64fb78bf707164eec56f7",
      "e0e5a3d2d91f4418a0bd9589f32186e9",
      "6334762a3c934c42b0633dcf0113f178",
      "238de15204b74da981f347dab541e875",
      "b1ea43a5a2bc4cb5b137ed30f69bfdc4",
      "60f0467934544e0b977fdc547af1ea3a",
      "4774849a97934c9c9bb9ecfb7cf2b2dd",
      "c399f1ae539a48af94475575e1fb1f90",
      "477b18af8eeb4077b8104f2d1309950e",
      "44758e1a9a6048bb8b642e420560dca7",
      "d6325a7a33d74e3dbf7365ccb8367544",
      "58cd05a91a344da2bdf8ad582b4ac1f2",
      "01c1a9fe94a44c6db1d453327a6056ae"
     ]
    },
    "id": "IE-jRdYE8sNC",
    "outputId": "eabc5fbe-3926-4785-d881-6f29c29e01b0"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Model A: teknium/OpenHermes-2.5-Mistral-7B...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "994798bbf8c5415f8afa8d01ca202a0e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "334e922a41c243278d93d665ff759145"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97c1510cd91c4a7e901ba4c0f24ec818"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee2b3ceae48444b6bdaf81254108f703"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6897a8bf34b64acb98ce29269a4c19bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3f061b4e75141f7986cf56103b5157d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f743e0ea37d4cc8b822d870567ec4a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Model B: Gryphe/MythoMist-7b...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/600 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "747fc5c42ddc4646be78f52783397bba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ef1082b288748e5a1cf2ec963a7a7a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77326c120bd54f0b9c516db082199455"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4990e6b7106a427aab10110662c3d495"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11d01e6bd20c42f9bce0541374b3fe17"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37def251d9d24d24b399acc7c7e97811"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90bdb3f2d9cc4c6f845bb57a67c5abb8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e06d3776e0fa4b0ca5ccd5b9e4193138"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29ba1e10bae74e8fa1d19d8d480d5e69"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0b8f1eaa94040e18f827ecd2ca6b18f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42a67f2e26bb484db8901bf463facccf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6334762a3c934c42b0633dcf0113f178"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting SLERP Merge with t=0.5...\n",
      "Shape mismatch at model.embed_tokens.weight: torch.Size([32002, 4096]) vs torch.Size([32000, 4096]). Skipping.\n",
      "Shape mismatch at lm_head.weight: torch.Size([32002, 4096]) vs torch.Size([32000, 4096]). Skipping.\n",
      "Merge complete. Saving model...\n",
      "Model successfully saved to: /content/drive/MyDrive/MyMergedModel01-7B\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#5 : TESTING THE MERGED MODEL"
   ],
   "metadata": {
    "id": "-KoGuHsQDTBw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Path to our saved model on Drive\n",
    "saved_model_path = \"/content/drive/MyDrive/MyMergedModel01-7B\"\n",
    "\n",
    "print(f\"Loading merged model from {saved_model_path}...\")\n",
    "\n",
    "# Load Model & Tokenizer\n",
    "# We use device_map=\"auto\" to put it on GPU for inference\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    saved_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "\n",
    "# Test Prompt\n",
    "prompt = \"Explain the concept of quantum entanglement to a 5-year-old.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Apply Chat Template (OpenHermes uses ChatML usually)\n",
    "# If the tokenizer doesn't have a chat_template, we format manually\n",
    "if tokenizer.chat_template:\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "else:\n",
    "    # Fallback manual formatting\n",
    "    input_text = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "print(\"Generating response...\")\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"-\" * 50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "8bd893c9109544aeb8f82d2f73948275",
      "edd42251cd9a4ff981890e308d1e43c7",
      "2561b58d03424f2389e64c3edf87d59e",
      "015cd1b4c5e64bd3932d909d732a9fed",
      "ad6b33d65c444984b6ee19f97cca9aab",
      "8cffc67dd14d4dd8b0d0021322763b63",
      "77410fce4f3f4806bf1c16c87c9d1881",
      "7635c570603144d2b3f6a4edc19b08f8",
      "5fa26c5c148e4cb4b163b818f8d60f05",
      "2c231f8edc2941dcb92eec6a4b40597b",
      "ff0f70d3c9ee424eabbddc4e55898b08"
     ]
    },
    "id": "fFPYfxhgDOby",
    "outputId": "5006ade1-176c-4831-ca27-fb73f61c458a"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading merged model from /content/drive/MyDrive/MyMergedModel01-7B...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bd893c9109544aeb8f82d2f73948275"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating response...\n",
      "--------------------------------------------------\n",
      "system\n",
      "You are a helpful assistant. \n",
      " user\n",
      "Explain the concept of quantum entanglement to a 5-year-old. \n",
      "\n",
      "Imagine you have two toys, let's say a teddy bear and a toy car. They are friends, and no matter how far apart they go, they always know what the other one is doing. Now, quantum entanglement is like this, but for tiny particles that we can't see with our eyes. These particles can be very far away from each other, but they still know what the other one is doing, even if we try to hide information from one of them. It's like they are connected by a special invisible thread that can't be broken, even if we move them to different ends of the universe. This is a very mysterious and fascinating thing that happens in the world of tiny particles.\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  }
 ]
}